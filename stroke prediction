{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1882037,"sourceType":"datasetVersion","datasetId":1120859}],"dockerImageVersionId":30139,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n---\ntitle: \"Group Assignment - Part Two\"\nauthor: \"S. Lurie, B. Barnard, X.X. Han, S. Bergeron\"\ndate: \"ALY 6040\"\noutput:\n  html_document:\n    toc: true\n    theme: united\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Introduction\nThis report is an analysis of a data set of hospital patients for the purpose of assigning risk factors to strokes and making recommendations to help prevent this event. Strokes are the second leading cause of death in the world according to the World Health Organization and are responsible for 11% of total deaths. Every year 15 million people suffer a stroke, 5 million of which pass away and another 5 million of which are left with a permanent disability (WHO, 2021). Many of these strokes are preventable through healthy habit forming and monitoring those at the highest risk can have a significant improvement in outcomes. For these reasons, it is an area that requires further study to prevent this event from impacting more lives than necessary. The following report focuses on determining risk factors for strokes and makes recommendations on how to prevent them. \n\n# Data Cleaning\nTo properly analyze the data and build models to make predictions, the first step is to clean the data. The vast majority of the data is already cleaned and ready for analysis. However, there are three columns in the data set that need to be cleaned. First, the one patient who identified as other was changed to be female. This was chosen as there were more females in the dataset than men. Additionally, the BMI and smoking preference variables needed cleaning. The BMI column is missing 3.93% of records and the smoking preference column is missing 30% of data. It was decided to input the mean value of 28.89 into the missing values in the BMI column. This approach had minimal impact on the variance due to the small percentage of missing records. \nIt was a different scenario on understanding the smoking preference data, for the team to decide on how to handle this non-numerical variable in terms of missing data. The team decided to handle this data inconsistency by taking the proportions of the smoking data from where it was not missing and applying the same proportions to fill in the missing data. The result of this cleaning strategy was far more desirable than removing the variable entirely, or than removing the records with unknown records, as there is potential for a link between smoking and a stroke. The downside of this approach, however, is that it does not consider the potential existing relationship between smoking and a stroke (or other variables) – meaning it is using random assignment. The additional randomness this method can create can have a potential impact on the results, and theoretically the experiment should be simulated multiple times in a Monte Carlo fashion in order to achieve best results. \n\n# Analysis \nAn initial look at our data set shows that it contains 5,111 patient records. Each record contains 11 numerical and categorical attributes to describe the patient. Some of the patient attributes that were collected were the patient’s gender, their age, whether or not they had hypertension, whether or not they had heart disease, if they had ever been married, what type of work they do, what type of residence area they live it, their average glucose levels, their body mass index (BMI), their smoking status, and if they had had a stroke or not. Out of all patient records in the data set, 249 patients had experienced a stroke and 4,861 had not. Patients ranged from newborns to 84 years old. 2,994 of the patients were females, 2,115 patients were male, and 1 patient identified as other. The average patient in the survey was 43.23, and the ages of patients appears to be close to normally distributed. \n\n## Early Takeaways and Expectations\nThe Exploratory Data Analysis and Data cleaning steps gave provided multiple insights and provided expectations going forward analyzing risk factors for a stroke. The first of these is the clear importance of age as a risk factor for a stroke – 24.9% of the strokes contained in the dataset belong to individual in the 75-79 year old age group, while only 4.01% were for patients under 45 years old. Additionally, it was found that Hypertension, a common risk factor for strokes per stroke.org, was greatly more prevalent in older patients, with 82.7% of cases occurring in those over the age of 50.\n\nEarly analysis also predicted that there is a correlation between higher average glucose levels and suffering strokes. The mean Average Glucose Level amount stroke patients was 132.54, where the data set mean was 106.15. Additionally, 18.88% of stroke patients had heart disease, and 26.51% had hypertension, implying that these are also likely risk factors for strokes. \n\nThe early expectations lead to the belief that Age, Hypertension, Heart Disease, and average glucose level are the most indicative risk factors for predicting a stroke based on this data. However, as only 4.87% of the dataset did experience a stroke, it is likely that the models will be inaccurate in being able to predict a stroke.\n\n## General Linearized Modeling\nThe first the team created was the general linearized model. The results from the model were quite interesting, and provided a new insight compared to exploratory data analysis. The model found that age, hypertension, average glucose levels and whether an individual worked for a private company or not had statistically significant impacts on whether an individual was likely to suffer a stroke. The new information here being the relationship between private company work and the potential for a stroke. In fact, the model found that chance of stroke greatly increased by working for a private company versus other forms of work. This likely does not indicate that people should not work for private companies, but more so indicates a relationship between stress and having a stroke. \n\nExploratory Data Analysis indicated that there was a significant relationship between aging and strokes, as well as relationships between having hypertension and high glucose levels with having a stroke. This makes it very easy to accept these findings as risk factors – despite poor model accuracy.\n\nExploratory data analysis did not indicate a relationship between work type and stroke, so it is quite interesting for this to be indicated as a significant risk factor by the Generalized Linear Model. This factor should be viewed skeptically going forward, as it has so far only appeared in one analysis tool. \n\n### Generalized Linear Modeling - - Interpretations and Recommendations\nWhile work type has not yet shown up in other models, this does not mean it can be ignored. As said earlier, the relationship is likely not actually with work type, but with the amount of stress an individual is experiencing. A good method for reducing strokes will likely be to focus on stress reduction techniques for those who work stressful jobs, which may be more common in the private sector than public, leading to this correlation seen in the generalized linear model. Methods to reduce stress can range from a stress ball, medication, screaming into a pillow, seeking professional help, or exercise. Any of these techniques, and many more, may help reduce an individual’s risk of experiencing a stroke. \n\nThe group does not recommend that an individual quits their job with the intent of reducing likelihood of a stroke, however, if someone is in a situation where they believe their job is causing more stress than they can handle, the individual should consider making a career move.  For the purposes of reducing other risk factors as well, the group recommends using the exercise method for stress reduction. Exercise can help lower body mass index, and with proper dieting, average glucose levels will reduce as well.\n\nThe model performed with poor accuracy, likely because it was not done with oversampled data, and there were so few stroke patients present in the dataset. An appropriate next step in this situation would be to rerun the analysis with oversampled data, to correct this factor imbalance. It was chosen to not take this approach for the first model attempt, as it should be based originally on real world findings, and then seen if it can be improved for other models by employing different methods. \n\n## Principal Components Analysis\nPrincipal Components Analysis (PCA) is a method of reducing data from many dimensions into two. This allows for a simple plot that can give insight into the structure of the data. From the results of variable importance for the PCA model, it shows the standard deviation, proportion of variance, and cumulative proportion of the variance 19 principal components calculated from the data after adjusting the data for dummy variables instead of categorical variables. First principal component (PC1) is the component that explains the greatest proportion of the variance, PC2 explains the second most, and so on. This is represented graphically in A2.\n\nThe cumulative variance plot in the 2D PCA plot shows the number of Principal Components s to include in the model to achieve a given level of variance the model explains. The dotted blue lines show that with 9 PC’s, 80% of the variance in the data can be explained. That is the level at which the model starts to become a viable predictor of stroke occurrence. Of course, it would be great to include all the available PC’s and predict stroke correctly 100% of the time, but that would lead to an overfit model with poor results when applied to a second dataset. A model containing fewer of the PC’s would avoid overfitting and can be viable predictor when applied to a new set of patient data. \n\nThe scatterplot in correlation charts the datapoints by PC1 and PC2. The scatterplot shows 12 distinct clusters. The blue and yellow circles represent the areas where the “Yes” and “No” values appear. The right side of the chart is largely devoid of “Yes” values. This indicates that a datapoint where a stroke occurred is much more likely to fall in an area where PC1 is a negative value.  \n    \nFinally, the results for the correlations between the PC’s and the different variables in the data set are also provided. The strongest correlations in PC1 are the variables for Age, Marital Status, and the dummy variable indicating that the patient is a child. The strongest correlations for PC2 are the variables for Gender, Smoking Status, and heart disease. These are the variables that have the greatest say in where the points are plotted on the 2D PCA Plot. It is interesting to note that these variables do not match with some of the more significant variables in previous model types. This would be an example of something that would be interesting to investigate when comparing the models outlined in this analysis.\n\n### Principal Components Analysis – Interpretations and Recommendations\nReducing the dimensions in an analysis can be incredibly useful for datasets where the number of dimensions available seems overwhelming. In this case, the number of variables in the dataset may not have been enough for Principal Components Analysis to be appropriate. Attempting to perform the analysis is not without merit, however. For example, the 2D PCA plot shows a clear clustering of data which can at least be considered one of the many clues uncovered from the several models tested here. Unfortunately, though, the accuracy of the 2D PCA Plot can be called into question, since PC1 and PC2 account for a relatively small amount of the cumulative variance in the data. \n\nThere are, however, some measures that can be taken moving forward to improve PCA model. One of these improvements may be a more thorough examination of potential multicollinearity issues in the data. Another may be to reduce the number of dummy variables in the dataset. The results on correlation of PCA with each variable shows the dummy variables for “male” and “female” entries have the same correlations with stroke with opposite signs. This may ultimately undermine the insights that can be gathered from the gender data. Subsequent models should dive deeper into the composition of the variables comprised in the PC’s.\n\n## Oversampling\nCreating accurate models for this data set requires that the team oversamples the data. This is necessary due to the large class imbalance in patients that had experienced a stroke compared to those that had not. Resampling works by duplicating existing data points for patients that had experienced a stroke. This reduces the imbalance between those that had experienced a stroke, and those that had not. If a model was created without oversampling the data, the model would be able to predict if a person will not have a stroke but would be almost useless for predicting if a person will have a stroke. The team decided to oversample the data instead of under sampling because our data set only contains 5,110 patient records. Under sampling is typically used for data sets with hundreds of thousands of records or more. The one downside of oversampling is that it risks overfitting the model. Additionally, oversampling the data changes the output of the model slightly. The variable importance for a model that uses oversampled data will be different than a model that uses the original data.\n\nThe team decided to create several different models in order to analyze the relationships between the variables in the data set and make predictions about whether or not someone will experience a stroke. By analyzing the variable relationships, we can better understand how these variables increase or decrease the risk that a patient will experience a stroke. The team will begin by presenting the model and explaining how it functions, the model will then be evaluated for accuracy and the variable importance in order to determine the stroke risk factors.\n\n## Random Forest Model\nThe final model that the team selected to make predictions by using the oversampled data was the random forest model. This model works by creating an ensemble out of many decision trees. Our model uses 400 trees in order to make predictions. The main reason why the team decided to use this model over a decision tree is because of its increased accuracy due to its complexity. The one drawback of choosing the random forest model over the decision tree model is that it takes slightly longer to make calculations due to its increased complexity. The random forest model was chosen to be presented last because unlike the previous two models, the random forest uses oversampled data to make predictions.\n\nThe model that the team created showed an overall accuracy of 97.43%. It is important to keep in mind when evaluating models with high class imbalances that accuracy likely won't be the most important performance characteristic to evaluate. To properly evaluate the model, it is important to pay close attention to sensitivity and specificity. Previous random forest models that were built without oversampling the data also achieved high accuracy, however the sensitivity rate (true positive rate) was very low making them essentially useless for predicting if someone will experience a stroke. This random forest model showed a high sensitivity rate of 100% as well as a high specificity rate of 96.3%. \n\nOne of the insights that the team was able to derive from evaluating the random forest model was the importance of the variables in terms of making predictions. By understanding which variables show a higher degree of importance, we can help to shed some light on our initial question which was about what factors increase the risk of stroke. The three most important variables in the random forest model were the age of the patient, their average blood glucose level, and their body mass index (BMI). This agrees with the exploratory data analysis performed, as the analysis indicated that these three variables would increase the risk of experiencing a stroke. \n\n### Random Forest - Interpretations and Recommendations\nAging is an inevitable part of life, there is nothing to be done preventatively in this area to reduce odds of suffering a stroke. It is, however, an important insight, as it is abundantly clear that as patients age, they must be ever more vigilant about the possibility of stroke and the measures that must be taken to prevent it. The main takeaway from this model is that as the patient ages, they should be more aware of stroke risk and actively communicating with their doctor regarding what other risk factors they have present. Additionally, they should be looking for if there is a history of stroke in their family, as genetics can often play a significant role in the health of individuals. \n\nThe next two highest indicators of stroke are much more actionable, as glucose level and body mass index are two things that can be regulated through diet and exercise. They can also be monitored regularly to ensure that those struggling with them are progressing toward healthier levels, which, combined with the knowledge of their significance, makes them great preventative tools. The recommendation on these factors also begins with a conversation with a professional, likely a doctor at first, but after the doctor a nutritionist or personal trainer. Speaking with these professionals, to work and reduce these risk factors as best as possible, will likely have a large impact in stroke prevention, particularly among the elderly. \n\n## Conclusion\nThe analysis and models were successful in finding the risk factors for strokes. The main takeaways of our analysis being that focusing on correct diet and exercise is big, especially as somebody gets older. Individuals who have a history of stroke in their family should be particularly careful to adhere and pay attention to guidelines around these risk factors. Anyone who has experience with any of these issues should speak with their primary care physician at next availability regarding stroke prevention. Additionally, there is some evidence to support that stress is related to strokes, and those in high stress environments should be aware of this risk and work to diminish their stress levels. Finally, our models indicated that individuals who suffer from Hypertension are additionally at risk and should be aware of this issue and speak with their physician about the risk from strokes. Hypertension was one of the variables that had high importance with several of the models that we ran, both with unsampled and original data. Those who are 50 years of age or older obviously cannot change their age, but they can focus on diminishing other risk factors such as hypertension, average glucose level, with methods such as healthy dieting, exercise, and including forms of stress reduction in their daily lives.\n\nOne of the major problems that the team experienced when analyzing this data set was that there was a large class imbalance between those that had experienced a stroke and those that had not. Additionally, this data set contains only 5,111 patient records. While oversampling may help the class imbalance issue, it does not eliminate it. Oversampling duplicates data points, meaning that it will have an affect the validity of the results. If the team had access to more data or more balanced data, better models could be produced, and they would likely be more consistent and accurate in terms of the outputs of the models. By having more consistent and accurate model outputs, the team could more accurately indicate the risk factors for stroke. \n\n# References\nA. (2019, December 25). GGPlot Legend Title, Position and Labels. Datanovia. https://www.datanovia.com/en/blog/ggplot-legend-title-position-and-labels/\n\nggplot2 axis ticks : A guide to customize tick marks and labels - Easy Guides - Wiki - STHDA. (n.d.). Statistical Tools for High-Throughput Data Analysis. Retrieved August 9, 2021, from http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels\n\nggplot2 colors : How to change colors automatically and manually? - Easy Guides - Wiki - STHDA. (n.d.). Statistical Tools for High-Throughput Data Analysis. Retrieved August 9, 2021, from http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually\n\nK FOLD Cross validation R Random forest : Machine learning. (2020, January 24). [Video]. YouTube. https://www.youtube.com/watch?v=M7BMgHwXUoY\n\nPackage “Rose.” (2021, June). CRAN. https://cran.r-project.org/web/packages/ROSE/ROSE.pdf\n\nNistrup, P., 2019. Principal Component Analysis (PCA) 101, using R. [online] Medium. https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff [Accessed 8 August 2021].\n\nVidhya, 2016. PCA: Practical Guide to Principal Component Analysis in R & Python. [online] Analytics Vidhya. https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/ [Accessed 8 August 2021].\n\n“Stroke Risk Factors.” Www.stroke.org, 2021, www.stroke.org/en/about-stroke/stroke-risk-factors. \n\n# Attaching requried package\n```{r}\n# Attach packages\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(data.tree)\nlibrary(caTools)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(ROSE)\nlibrary(hrbrthemes)\nlibrary(caret)\nlibrary(olsrr)\nlibrary(cvms)\nlibrary(tibble) \nlibrary(MASS)\nlibrary(pROC)\nlibrary(DescTools)\nlibrary(randomForest)\nlibrary(factoextra)\n```\n\n# Data Cleaning - From EDA Group Assignment Report\n```{r}\n# Import dataset\nlocation <- '../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'\ndata0 <- read.csv(location)\n\n# summarize the data\nsummary(data0)\n```\n```{r}\n# View data classes\nstr(data0)\n\n# Duplicate Data\ndata1 <- data0\n\n# View all distinct categorical variable\nlapply(subset(data1, select = c(gender, ever_married, work_type, Residence_type, bmi, smoking_status)), unique)\n```\n## Part 1 - BMI\n```{r}\n# Check data type\nclass(data1$bmi)\n\n# Convert BMI to numeric\ndata1$bmi <- as.numeric(data1$bmi)\n\n# Check data type\nclass(data1$bmi)\n\n# View Summary Statistics for data\nsummary(data1$bmi)\n\n# Replace N/A's in BMI column with mean\ndata1$bmi[is.na(data1$bmi)] <- mean(data1$bmi,na.rm=TRUE)\n\n# View New Summary Statistics for data\nsummary(data1$bmi)\n```\n## Part 2 - Gender\n```{r}\n# Count the unique variables in the gender column\ntable(data1$gender)\n\n# As 'Female' has the majority counts, replace 'Other' to 'Female'\ndata1$gender <- ifelse(data1$gender == \"Other\", \"Female\", data1$gender)\n\n# Count the unique variables in the revised gender column\ntable(data1$gender)\n```\n## Part 3 - Smoking Status\n```{r}\n# Count the unique variables in the gender column\ntable(data1$smoking_status)\n\n# Calculate the probability of formerly smoker, current smokers and non-smokers given that there's only this three categories in the smoking_status column\nprob.FS <- 885 / (885 + 1892 + 789)\nprob.NS <- 1892 / (885 + 1892 + 789)\nprob.S <- 789 / (885 + 1892 + 789)\n\n# Duplicate the data\ndata2 <- data1\n```\n\n```{r}\n# Replacing 'Unknown' in smoking_status by the other 3 variables according to their weightage\ndata2$rand <- runif(nrow(data2))\ndata2 <- data2%>%mutate(Probability = ifelse(rand <= prob.FS, \"formerly smoked\", ifelse(rand <= (prob.FS+prob.NS), \"never smoked\", ifelse(rand <= 1, \"smokes\", \"Check\"))))\ndata2 <- data2%>%mutate(smoking.status = ifelse(smoking_status == \"Unknown\", Probability, smoking_status))\n# View the new Smoking Status column's unique values and their counts\ntable(data2$smoking.status)\n# Remove columns that are not needed\nhealth <- subset(data2, select = -c(rand,Probability,smoking_status))\n# revise the column name of smoking status\ncolnames(health)[12] <- \"smoking_status\"\n# 'health' is the final modified dataset which will be used for the EDA section below.\n```\n\n```{r}\n# view the first 10 rows\nhead (health,10)\n# Plot the graph\nggplot(health, aes(color=smoking_status, x=age, y=bmi)) + geom_point()\n```\n\n## Part 4 - ID\n```{r}\n# Change ID to Null\nclass(health$id)\nhealth$id <- NULL\nclass(health$id)\nhead(health$id, 3)\n```\n# Exploratory Data Analysis\n\n## Create a Subset\n```{r}\n# Subset Data into Yes and No (stroke)\nYes <- subset(health, stroke == '1')\nNo <- subset(health, stroke == '0')\n```\n\n## Part 1 - Bar Charts\n```{r,fig.height = 3, fig.width = 3}\nlibrary(\"ggplot2\")\n# Create stroke counts table\nstrokecounts <- as.data.frame(table(health$stroke))\n\n# As 'Female' has the majority counts, replace num to char \nstrokecounts$Var1 <- ifelse(strokecounts$Var1 == 0, \"No\", 'Yes') \n\n# Bar Chart of Patients With and Without Strokes\nggplot(strokecounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Stroke Status of Patients\",x =\"Stroke\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\nThe number of patients who have not had strokes is vastly greater than the number of patients who have.\n\n```{r,fig.height = 4, fig.width = 4}\n# Create hypertension counts table\nhypercounts <- as.data.frame(table(health$hypertension, health$stroke))\n  \n# Replace num to char\nhypercounts$Var1 <- ifelse(hypercounts$Var1 == 0, \"No\", 'Yes')\nhypercounts$Var2 <- ifelse(hypercounts$Var2 == 0, \"No\", 'Yes')\n\n# Replace headers\ncolnames(hypercounts)[1] <- 'Hypertension'\ncolnames(hypercounts)[2] <- 'Stroke'\n\n# Bar Chart of Hypertension : No vs. Yes     \nggplot(hypercounts, aes(x = Hypertension, y = Freq, fill = Stroke)) +\n        geom_bar(stat = \"identity\")+\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Hypertension Status of Patients\",x =\"Hypertension\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n\n```\n\nThe number of patients without hypertension is vastly greater than the number of patients with hypertension, but the gap is slightly less than the gap seen for stroke victims.\n\n```{r,fig.height = 4, fig.width = 4}\n# Create heart disease counts table\nheartcounts <- as.data.frame(table(health$heart_disease, health$stroke))\n\n# Replace num to char\nheartcounts$Var1 <- ifelse(heartcounts$Var1 == 0, \"No\", 'Yes')\nheartcounts$Var2 <- ifelse(heartcounts$Var2 == 0, \"No\", 'Yes')\n\n# Replace headers\ncolnames(heartcounts)[1] <- 'Heart_Disease'\ncolnames(heartcounts)[2] <- 'Stroke'\n\n# Bar Chart of Heart Disease : No vs. Yes     \nggplot(heartcounts, aes(x = Heart_Disease, y = Freq, fill = Stroke)) +\n        geom_bar(stat = \"identity\") + \n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Heart Disease Status of Patients\",x =\"Heart Disease\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\nThe gap between patients with and without heart disease more closely resembles the gap between those with and without strokes.\n\n```{r,fig.height = 4, fig.width = 4}\n# Create gender counts table\ngendercounts <- as.data.frame(table(health$gender))\n  \n# Bar Chart of Gender   \nggplot(gendercounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Gender of Patients\",x =\"Gender\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\n\nThere are more Female patients than Male. The one entry that was stated as 'Other' was added to the 'Female' section since majority are female patients.\n\n```{r,fig.height = 4, fig.width = 7}\n# Create work type counts table\nworkcounts <- as.data.frame(table(health$work_type))\n  \n# Bar Chart of Patient Work Type   \nggplot(workcounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Patient Work Type\",x =\"Work Type\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\nThere are approximately even amounts of patients that are working government jobs, are self-employed, and are children. The majority of patients work for private companies, and a small number have never worked.\n\n```{r,fig.height = 4, fig.width = 4}\n# Create ever married counts table\nmarriedcounts <- as.data.frame(table(health$ever_married))\n  \n# Bar Chart of Patients Who Have Been Married  \nggplot(marriedcounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Bar Chart of Patients Who Have Been Married\",x =\"Have the Patient been Married\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\nRoughly double the amount of patients have been married before than those who have not. \n\n```{r,fig.height = 4, fig.width = 4}\n# Create residence type counts table\nrescounts <- as.data.frame(table(health$Residence_type))\n  \n# Bar Chart of Patients Who Have Been Married  \nggplot(rescounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Residence Type of the Patients\",x =\"Residence Type\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\nThe patients are nearly evenly distributed between rural and urban residences\n\n```{r,fig.height = 4, fig.width = 4}\n# Create smoking status counts table\nsmokecounts <- as.data.frame(table(health$smoking_status))\n  \n# Bar Chart of Patients Who Have Been Married  \nggplot(smokecounts, aes(x = Var1, y = Freq, fill = Var1)) +\n        geom_bar(stat = \"identity\") + theme(legend.position=\"none\") +\n        geom_text(aes(label = Freq), vjust = 0) +\n        labs(title=\"Smoking Status of Patients\",x =\"Smoking Status\", y = \"Frequency\") +\n        theme(plot.title = element_text(hjust = 0.5))\n```\n\n\nThe unknown data was randomly added to the three categories above based of the probability. Most patients have either never smoked. The data for formerly and currently smokers are similar.\n\n## Part 2 - Histograms\n\n```{r,fig.height = 8, fig.width = 12}\n# Histogram of Age with normal distribution overlay\nhistage <- hist(health$age,xlim=c(0,100),\n                main=\"Histogram of Age with Normal Distribution Overlay\",\n                xlab=\"Age\",las=1)\nxfit <- seq(min(health$age),max(health$age))\nyfit <- dnorm(xfit,mean=mean(health$age),sd=sd(health$age))\nyfit <- yfit*diff(histage$mids[1:2])*length(health$age)\nlines(xfit,yfit,col=\"red\",lwd=2)\n```\n\nThe ages of the patients in the study are close to a normal distribution, with mean of 43.23 from the summary() function. Based on the information from the summary() function earlier and the chart above, most patients are around their 40s.\n\n```{r,fig.height = 8, fig.width = 12}\n# Histogram of Average Glucose Level with normal distribution overlay\nhistglucose <- hist(health$avg_glucose_level,xlim=c(0,300),\n                main=\"Histogram of Avg. Glucose with Normal Distribution Overlay\",\n                xlab=\"Avg. Glucose\",las=1)\nxfit <- seq(min(health$avg_glucose_level),max(health$avg_glucose_level))\nyfit <- dnorm(xfit,mean=mean(health$avg_glucose_level),sd=sd(health$avg_glucose_level))\nyfit <- yfit*diff(histglucose$mids[1:2])*length(health$avg_glucose_level)\nlines(xfit,yfit,col=\"red\",lwd=2)\n```\n\nThe average glucose levels of the patients in the study are right skewed, with mean of 106.15 from the summary() function earlier.\n\n```{r,fig.height = 8, fig.width = 12}\n# Histogram of BMI with normal distribution overlay\nhistbmi <- hist(health$bmi,xlim=c(0,100),\n                main=\"Histogram of BMI with Normal Distribution Overlay\",\n                xlab=\"Body Mass Index\",las=1)\nxfit <- seq(min(health$bmi),max(health$bmi))\nyfit <- dnorm(xfit,mean=mean(health$bmi),sd=sd(health$bmi))\nyfit <- yfit*diff(histbmi$mids[1:2])*length(health$bmi)\nlines(xfit,yfit,col=\"red\",lwd=2)\n```\n\nThe data for patient Body Mass Index is right skewed, with a mean of 28.89 from the summary() function above after modification. All NAs were updated to mean in Data Cleaning- Part 1 Section.\n\n## Part 3 - Boxplot\n\n```{r}\n# Box and Whisker Plot of Average Glucose in Patients With and Without Strokes\nboxplot(Yes$avg_glucose_level,No$avg_glucose_level,\n        main=\"Boxplot of Average Glucose Level by Stroke Status\",\n        ylab=\"Average Glucose Level\",las=1,names=c(\"Stroke\",\"No Stroke\"))\n```\n\nThe boxplot shows a relatively similar mean average glucose level in patients who suffered strokes and patients who have not, with lots of high outliers among non-stroke victims.\n\n```{r}\n# Box and Whisker Plot of Body Mass Index in Patients With and Without Strokes\nboxplot(Yes$bmi,No$bmi,main=\"Boxplot of Body Mass Index by Stroke Status\",\n        ylab=\"BMI\",las=1,names=c(\"Stroke\",\"No Stroke\"))\n```\n\nThe boxplot shows a relatively similar mean Body Mass Index in patients who suffered strokes and patients who have not, with a few high outliers among stroke victims and many high outliers among non-stroke victims.\n\n## Part 4 - Violinplot\n\n```{r}\n# Create a temp table for violin plots\ntemp <- health\n# Replace num to char\ntemp$stroke <- ifelse(temp$stroke == 0, \"No\", 'Yes')\n\n# Violin Plot of Age in Patients With and Without Strokes\nggplot(temp, aes(x=stroke, y=age, fill = stroke)) +\n  geom_violin(trim=FALSE, fill='#A4A4A4', color=\"darkred\")+\n  geom_boxplot(width=0.1) + theme_minimal()\n\n```\n\nThe violin plot shows a much higher mean age in patients who suffered strokes than in those who have not, with a pair of low outliers among stroke victims. The older the patient is, there is a higher likelihood to be diagnosed with stroke.\n\n```{r}\n# Replace num to char\ntemp$heart_disease <- ifelse(temp$heart_disease == 0, \"No\", 'Yes')\n\n# Violin Plot of Age in Patients With and Without Heart Disease\nggplot(temp, aes(x=heart_disease, y=age, fill = heart_disease)) +\n  geom_violin(trim=FALSE, fill='#A4A4A4', color=\"darkred\")+\n  geom_boxplot(width=0.1) + theme_minimal()\n```\n\nThe violin plot shows a much higher mean age in patients who suffered heart disease than in those who have not, with a pair of low outliers among stroke victims. The older the patient is, there is a higher likelihood to be diagnosed with heart disease.\n\n```{r}\n# Replace num to char\ntemp$hypertension <- ifelse(temp$hypertension == 0, \"No\", 'Yes')\n# Violin Plot of Age in Patients With and Without Hypertension\nggplot(temp, aes(x=hypertension, y=age, fill = hypertension)) +\n  geom_violin(trim=FALSE, fill='#A4A4A4', color=\"darkred\")+\n  geom_boxplot(width=0.1) + theme_minimal()\n```\n\n\nThe violin plot shows a much higher mean age in patients who suffered hypertension than in those who have not, with a pair of low outliers among stroke victims. The older the patient is, there is a higher likelihood to be diagnosed with hypertension.\n\n## Part 5 - Correlograms\n\n### Correlogram of the Numeric Variables\n\nThe chart below  shows the correlation of all numerical variables in the cleaned data. The values in the diagonal cells are the min and max values. As an example, the minimum bmi is 10.3 whereas the highest bmi is 97.6. From the correlogram and the correlation table, it shows that all numeric variables are positively correlated to the predictor variable [stroke]. Age has the highest correlation index to stroke. \n\n```{r}\nlibrary(corrgram)\n# Create the correlogram for numerical variables\ncorrgram(health, order=NULL, panel=panel.shade, text.panel=panel.txt,\n         diag.panel=panel.minmax, main=\"Correlogram\") \n# View the 2 decimal point correlation values of numerical variables\nround(cor(subset(health, select=c(age,hypertension, heart_disease,avg_glucose_level, bmi, stroke))),2) \n```\n\n### Correlogram of the Numeric Variables and Categorical Variables\nFrom the correlation matrix based on numerical variables, there are 4 driving factors to determine if a patient will be diagnosed with stroke. They are age, hypertension, heart_disease and avg_glucose_level.\n\nThe plot below includes both numerical and categorical variables. The the top 4 driving factors remains the same as above. Ever_married has the highest correlation with stroke among all the categorical variables.\n\n```{r}\nlibrary(caret)\nlibrary(corrr)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Convert the Categorical Variables to Numerical Variables\n# The new dataset is named as onehot\ndmy <- dummyVars(\" ~ .\", data = health)\nonehot <- data.frame(predict(dmy, newdata = health))\n# View the headers of the new dataset\nnames(onehot)\n\n# Correlation Table\ncor_onehot <- correlate(onehot)\n# Extract correlation related to stroke\ncor_onehot%>% focus(stroke)\n\n# Plot the correlation between stroke and all others\ncor_onehot %>%\n  focus(stroke) %>%\n  mutate(rowname = reorder(term, stroke)) %>%\n  ggplot(aes(term, stroke)) +\n    geom_col() + coord_flip() +\n  theme_bw()\n```\n\n### Part 6 - Bar Chart grouped by Age\n\n```{r}\nstroke <- health$stroke\nhypertension <- health$hypertension\nbmi <- health$bmi\n#Agegroups by stroke\nhealth$AgeGroup <- cut(health$age, \n                         breaks = c(-Inf\n                                    ,5 ,10 ,15,20,25,30,35,40,45,50,55,60 ,65,70,75,80\n                                    , Inf), \n                         \n                         labels = c(\"0-4\"\n                                    ,\"5-9\",\"10-14\",\"15-19\",\"20-24\"\n                                    ,\"25-29\",\"30-34\",\"35-39\",\"40-44\"\n                                    ,\"45-49\",\"50-54\",\"55-59\",\"60-64\"\n                                    ,\"65-69\",\"70-74\",\"75-79\",\"80-84\"\n                                    ),\n                         right = FALSE)\ntable(stroke)\ntable(hypertension, health$AgeGroup)\ntable(hypertension)\n```\n```{r,fig.height = 6, fig.width = 20}\n#AgeGroup\nagetablestroke <- as.data.frame(table(health$AgeGroup, stroke))\nagetablestroke\nggplot(agetablestroke, aes(x=Var1, y=Freq, fill=stroke)) + geom_bar(stat=\"identity\") +\n  theme_ipsum() + scale_x_discrete(name = \"Age Group\") + \n  ggtitle(\"Patient Age Group by Stroke Occurence\") + ylab(\"Number of Patients\\n\") +\n  scale_fill_brewer(palette=\"Paired\", labels=c(\"No\",\"Yes\")) +\n  theme(axis.title.x = element_text(face=\"bold\", size=14, hjust = 0.5),\n        axis.title.y = element_text(face=\"bold\", size=20, hjust=0.5),\n        axis.text.x = element_text(face=\"bold\", size=16),\n        axis.text.y = element_text(face=\"bold\", size=18),\n        legend.text = element_text(face = \"bold\", size=12),\n        legend.title = element_text(face = \"bold\", size=16))\n```\n\n```{r,fig.height = 6, fig.width = 20}\n#Agegroup by hypertension\nagetablehypertension <- as.data.frame(table(health$AgeGroup, hypertension))\nagetablehypertension\n\nggplot(agetablehypertension, aes(x=Var1, y=Freq, fill=hypertension)) + geom_bar(stat=\"identity\") +\n  theme_ipsum() + scale_x_discrete(name = \"Age Group\") + \n  ggtitle(\"Patient Age Group by Hypertension Occurence\") + ylab(\"Number of Patients\\n\") +\n  scale_fill_brewer(palette=\"Paired\", labels=c(\"No\",\"Yes\")) +\n  theme(axis.title.x = element_text(face=\"bold\", size=14, hjust = 0.5),\n        axis.title.y = element_text(face=\"bold\", size=20, hjust=0.5),\n        axis.text.x = element_text(face=\"bold\", size=16),\n        axis.text.y = element_text(face=\"bold\", size=18),\n        legend.text = element_text(face = \"bold\", size=12),\n        legend.title = element_text(face = \"bold\", size=16),\n        plot.title = element_text(face=\"bold\", size = 22))\n```\n\n```{r,fig.height = 6, fig.width = 20}\n#Smoking status \nsmoke <- as.data.frame(table(health$smoking_status, stroke))\nsmoke\nggplot(smoke, aes(x=Var1, y=Freq, fill=stroke)) + theme_ipsum() + \n  scale_x_discrete(name = \"BMI\") + geom_bar(position=\"dodge\", stat=\"identity\") + \n  ggtitle(\"Patient BMI by Stroke Occurence\")+\n  theme(axis.title.x = element_text(face=\"bold\", size=14, hjust = 0.5),\n        axis.title.y = element_text(face=\"bold\", size=20, hjust=0.5),\n        axis.text.x = element_text(face=\"bold\", size=16),\n        axis.text.y = element_text(face=\"bold\", size=18),\n        legend.text = element_text(face = \"bold\", size=12),\n        legend.title = element_text(face = \"bold\", size=16),\n        plot.title = element_text(face=\"bold\", size = 22))\n\nbmistroke <- as.data.frame(table(bmi, stroke))\n```\n# Modeling\n## Data Spilt using Original Data\n\n```{r}\nhealth$gender <- as.factor(health$gender)\nhealth$stroke <- as.factor(health$stroke)\ntable(health$gender)\nclass(health$stroke)\n```\n```{r}\nset.seed(100)\nhealth$AgeGroup <-NULL\nsample = sample.split(health$stroke, SplitRatio = 0.7)\ntrain = subset(health, sample==TRUE)\ntest = subset(health, sample==FALSE)\nhealth$gender <- as.factor(health$gender)\ntable(health$gender)\n```\n\n## Creating Dummy Variables\n```{r}\nlibrary(fastDummies)\n# Duplicate Data\nhealthdummy <- health\n# Creating dummy columns\nhealthdummy <- dummy_cols(healthdummy,select_columns=c(\"gender\",\"ever_married\",\n                          \"work_type\",\"Residence_type\",\"smoking_status\"),\n                          remove_selected_columns = TRUE)\n\n# View a summary of the data\nsummary(healthdummy)\nstr(healthdummy)\n\n# Replace factor to num\nhealthdummy$stroke <- ifelse(healthdummy$stroke == '1', 1, 0)\ntable(healthdummy$stroke)\n\nstr(healthdummy)\n\nset.seed(123)\ntrainIndex <- sample(x=nrow(healthdummy),size=nrow(healthdummy)*0.7)\nhealthdummytrain <- healthdummy[trainIndex,]\nhealthdummytest <- healthdummy[-trainIndex,]\n```\n\n\n## Model 1: Generalized Linear Model - Logistic Regression -DummyVar\nAs our dataset produces binary output. We will use logistic regression instead of linear regression\n```{r}\nglm_fit <- glm(stroke~., data=healthdummytrain, family = binomial)\nsummary(glm_fit)\n```\n\n```{r}\nstepdummy <- stepAIC(glm_fit)\nsummary(stepdummy)\n```\n\n```{r}\n# Make predictions\nprobabilities <- stepdummy %>% predict(healthdummytest, type = \"response\")\npredicted_classes <- ifelse(probabilities > 0.5, 1, 0)\n# Model accuracy\nprint(paste(\"Model Accuracy : \", mean(predicted_classes == healthdummytest$stroke)))\n# Confusion Matrix\ntable(healthdummytest$stroke, probabilities >= 0.5)\n# Anova Table\nanova(stepdummy)\n```\n\n## Model 2A: Principal Components Analysis -DummyVar\n\n```{r}\nhealthdummy <- healthdummy[, c(6, 1, 2, 3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20)]\n# Create PCA Model\nPCA <- prcomp(healthdummy[c(2:20)], center = TRUE, scale=TRUE)\n\n# View Summary of PCA Model\nsummary(PCA)\n\n# Extract PC Scores\nstr(PCA)\nhealthdummy2 <- cbind(healthdummy,PCA$x[,1:9])\nhead(healthdummy2,3)\n\nx <- cor(healthdummy, PCA$x[,1:9])\nsummary(x)\nx\n```\n\n```{r}\n# Screeplot\nscreeplot(PCA, type = \"l\", npcs = 15, main = \"Screeplot of the first 15 PCs\")\nabline(h = 1, col=\"red\", lty=5)\nlegend(\"topright\", legend=c(\"Eigenvalue = 1\"),\n       col=c(\"red\"), lty=5, cex=0.6)\n```\n\n```{r}\ncumpro <- cumsum(PCA$sdev^2 / sum(PCA$sdev^2))\nplot(cumpro[0:15], xlab = \"PC #\", ylab = \"Amount of explained variance\", main = \"Cumulative variance plot\")\nabline(v = 9, col=\"blue\", lty=5)\nabline(h = 0.82437, col=\"blue\", lty=5)\nlegend(\"topleft\", legend=c(\"Cut-off @ PC9\"),\n       col=c(\"blue\"), lty=5, cex=0.6)\n```\n\n\n```{r}\nplot(PCA$x[,1],PCA$x[,2], xlab=\"PC1 (19.16%)\", ylab = \"PC2 (10.99%)\", main = \"PC1 / PC2 - plot\")\n```\n```{r}\nhealthdummy$stroke <- ifelse(healthdummy$stroke == 0, \"No\", \"Yes\")\nfviz_pca_ind(PCA, geom.ind = \"point\", pointshape = 21, \n             pointsize = 2, \n             fill.ind = healthdummy$stroke, \n             col.ind = \"black\", \n             palette = \"jco\", \n             addEllipses = TRUE,\n             label = \"var\",\n             col.var = \"black\",\n             repel = TRUE,\n             legend.title = \"Diagnosis\") +\n  ggtitle(\"2D PCA-plot from 19 feature dataset\") +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n\n## Model 2B: Generalized Linear Model - Logistic Regression -DummyVar_PCA\n\n```{r}\nhealthdummy$stroke <- ifelse(healthdummy$stroke == \"No\", 0, 1)\n```\n```{r}\n#add a training set with principal components\ndata_pca <- data.frame(stroke = healthdummy$stroke, PCA$x)\ndata_pca <- data_pca[,1:10]\n\nset.seed(123)\ntrainIndex <- sample(x=nrow(data_pca),size=nrow(data_pca)*0.7)\ntraindata_pca <- data_pca[trainIndex,]\ntestdata_pca <- data_pca[-trainIndex,]\n```\n\n```{r}\nglm_fit_pca <- glm(stroke~., data=traindata_pca, family = binomial)\nsummary(glm_fit_pca)\n```\n\n```{r}\nstepdummy_pca <- stepAIC(glm_fit_pca)\nsummary(stepdummy_pca)\n```\n\n```{r}\n# Make predictions\nprobabilities <- stepdummy_pca %>% predict(testdata_pca, type = \"response\")\npredicted_classes <- ifelse(probabilities > 0.5, 1, 0)\n# Model accuracy\nprint(paste(\"Model Accuracy : \", mean(predicted_classes == testdata_pca$stroke)))\n# Confusion Matrix\ntable(testdata_pca$stroke, probabilities >= 0.5)\n# Anova Table\nanova(stepdummy_pca)\n```\n\n## Data Split using Oversampling Data\n```{r}\n#Oversampling data\nhealth$AgeGroup <- NULL\nhealthoversample <- ovun.sample(stroke~.,data = health, method = 'over',p = 0.3)$data\n\noversampletable <- as.data.frame(table(healthoversample$stroke))\nregularsampletable <- as.data.frame(table(health$stroke))\n\n#Before adjustment\nggplot(regularsampletable, aes(x=Var1, y=Freq, fill=Var1)) + geom_bar(stat = 'identity') +\n  theme_tinyhand() + xlab(\"\\nDid the patient experience a stroke?\") + ylab(\"Patients\\n\") +\n  ggtitle(\"Number of Patients by Stroke Occurence\") + \n  scale_fill_manual(breaks = c(\"0\", \"1\"), \n                    values=c(\"darkgreen\",\"red3\"), name = \"Stroke Occured\", labels = c(\"No\",\"Yes\")) +\n  theme(axis.text.x = element_text(size=18),\n        legend.text = element_text(size=14),\n        legend.title = element_text(size=14),\n        axis.text.y = element_text(size=14),\n        axis.title.y = element_text(size=18, hjust=0.5),\n        axis.title.x = element_text(size=18, hjust=0.5)) +\n  scale_x_discrete(labels=c(\"0\" = \"No\", \"1\" = \"Yes\"))\n\n#After adjustment\nggplot(oversampletable, aes(x=Var1, y=Freq, fill=Var1)) + geom_bar(stat = 'identity') +\n  theme_tinyhand() + xlab(\"\\nDid the patient experience a stroke?\") + ylab(\"Patients\\n\") +\n  ggtitle(\"Number of Patients by Stroke Occurence (Oversampled)\") + \n  scale_fill_manual(breaks = c(\"0\", \"1\"), \n                    values=c(\"darkgreen\",\"red3\"), name = \"Stroke Occured\", labels = c(\"No\",\"Yes\")) +\n  theme(axis.text.x = element_text(size=18),\n        legend.text = element_text(size=14),\n        legend.title = element_text(size=14),\n        axis.text.y = element_text(size=14),\n        axis.title.y = element_text(size=18, hjust=0.5),\n        axis.title.x = element_text(size=18, hjust=0.5)) +\n  scale_x_discrete(labels=c(\"0\" = \"No\", \"1\" = \"Yes\"))\n```\n\n```{r}\nhealthoversample$gender <- as.factor(healthoversample$gender)\nhealthoversample$stroke <- as.factor(healthoversample$stroke)\ntable(healthoversample$gender)\nclass(healthoversample$stroke)\n\nset.seed(100)\nhealthoversample$AgeGroup <-NULL\nsample_os = sample.split(healthoversample$stroke, SplitRatio = 0.7)\ntrain_os = subset(healthoversample, sample==TRUE)\ntest_os = subset(healthoversample, sample==FALSE)\nhealthoversample$gender <- as.factor(healthoversample$gender)\ntable(healthoversample$gender)\n```\n\n## Model 5:  Random Forest Model- Oversampling Data\n```{r}\n#Random Forest Model\nRandom_Forest_Model_os <- randomForest(stroke~., data=train_os, ntree=500)\nRandom_Forest_Model_os\n\nprobabilities.train_os <- predict(Random_Forest_Model_os, train_os, type='class')\nconfusionMatrix(probabilities.train_os, train_os$stroke, positive = '1')\n\nprobabilities.test_os <- predict(Random_Forest_Model_os, test_os, type='class')\nconfusionMatrix(probabilities.test_os, test_os$stroke, positive = '1')\n\n#Random Forest Evaluation\noob.error.data_os <- data.frame(\n  Trees=rep(1:nrow(Random_Forest_Model_os$err.rate), times=3),\n  Type=rep(c(\"OOB\",\"0\",\"1\"), each=nrow(Random_Forest_Model_os$err.rate)),\n  Error=c(Random_Forest_Model_os$err.rate[,\"OOB\"],\n          Random_Forest_Model_os$err.rate[,\"0\"],\n          Random_Forest_Model_os$err.rate[,\"1\"])\n)\n\nggplot(data = oob.error.data_os, aes(x=Trees, y=Error)) + geom_line(aes(color=Type)) + theme_ipsum() +\n  ggtitle(\"Error Rate by Number of Trees\") + xlab(\"\\nNumber of Trees\") +\n  ylab(\"Error Rate\\n\") +\n  theme(axis.title.x = element_text(face=\"bold\", size=14, hjust = 0.5),\n        axis.title.y = element_text(face=\"bold\", size=14, hjust=0.5),\n        axis.text.x = element_text(face=\"bold\", size=16),\n        axis.text.y = element_text(face=\"bold\", size=16),\n        legend.text = element_text(face = \"bold\", size=12),\n        legend.title = element_text(face = \"bold\", size=15))\n\nvarImpPlot(Random_Forest_Model_os)\n```\n\n## Model 6: Random Forest Model - Original Data\n\n```{r}\n#Random Forest Model----ORIGINAL DATA----\nRandom_Forest_Model <- randomForest(stroke~., data=train, ntree=500)\nRandom_Forest_Model\n```\n\n```{r}\n# Confusion Matris and Statistics\nprobabilities.train <- predict(Random_Forest_Model, train, type='class')\nconfusionMatrix(probabilities.train, train$stroke, positive = '1')\n```\n\n```{r}\nprobabilities.test <- predict(Random_Forest_Model, test, type='class')\nconfusionMatrix(probabilities.test, test$stroke, positive = '1')\n\nconf_mat <- confusion_matrix(targets = test$stroke,\n                             predictions = probabilities.test)\nconf_mat\nplot_confusion_matrix(conf_mat)\n```\n\n```{r}\n#Random Forest Evaluation\noob.error.data <- data.frame(\n  Trees=rep(1:nrow(Random_Forest_Model$err.rate), times=3),\n  Type=rep(c(\"OOB\",\"0\",\"1\"), each=nrow(Random_Forest_Model$err.rate)),\n  Error=c(Random_Forest_Model$err.rate[,\"OOB\"],\n          Random_Forest_Model$err.rate[,\"0\"],\n          Random_Forest_Model$err.rate[,\"1\"])\n)\n\nggplot(data = oob.error.data, aes(x=Trees, y=Error)) + geom_line(aes(color=Type)) + theme_ipsum() +\n  ggtitle(\"Error Rate by Number of Trees\") + xlab(\"\\nNumber of Trees\") +\n  ylab(\"Error Rate\\n\") +\n  theme(axis.title.x = element_text(face=\"bold\", size=14, hjust = 0.5),\n        axis.title.y = element_text(face=\"bold\", size=14, hjust=0.5),\n        axis.text.x = element_text(face=\"bold\", size=16),\n        axis.text.y = element_text(face=\"bold\", size=16),\n        legend.text = element_text(face = \"bold\", size=12),\n        legend.title = element_text(face = \"bold\", size=15))\n```\n```{r}\nvarImpPlot(Random_Forest_Model)\n```","metadata":{"_uuid":"19d7d35c-e275-4234-8d4e-1bdd776cd6c5","_cell_guid":"2527453b-8f35-4904-85e2-ff4854f3ad1c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}